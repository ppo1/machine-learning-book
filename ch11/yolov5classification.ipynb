{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5  # clone\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nimport torch\nimport utils\ndisplay = utils.notebook_init()  # checks","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-21T14:52:02.827248Z","iopub.execute_input":"2023-07-21T14:52:02.827713Z","iopub.status.idle":"2023-07-21T14:52:36.204230Z","shell.execute_reply.started":"2023-07-21T14:52:02.827666Z","shell.execute_reply":"2023-07-21T14:52:36.203244Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"YOLOv5 ðŸš€ v7.0-193-g485da42 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n","output_type":"stream"},{"name":"stdout","text":"Setup complete âœ… (2 CPUs, 15.6 GB RAM, 4958.8/8062.4 GB disk)\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd ../yolov5\nfrom utils.downloads import attempt_download\n\np5 = ['n', 's', 'm', 'l', 'x']  # P5 models\ncls = [f'{x}-cls' for x in p5]  # classification models\n\nfor x in cls:\n    attempt_download(f'weights/yolov5{x}.pt')","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:52:36.206605Z","iopub.execute_input":"2023-07-21T14:52:36.207575Z","iopub.status.idle":"2023-07-21T14:52:42.220696Z","shell.execute_reply.started":"2023-07-21T14:52:36.207537Z","shell.execute_reply":"2023-07-21T14:52:42.219748Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5\n","output_type":"stream"},{"name":"stderr","text":"Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-cls.pt to weights/yolov5n-cls.pt...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.87M/4.87M [00:00<00:00, 14.1MB/s]\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to weights/yolov5s-cls.pt...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.5M/10.5M [00:00<00:00, 72.1MB/s]\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-cls.pt to weights/yolov5m-cls.pt...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.9M/24.9M [00:00<00:00, 41.9MB/s]\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-cls.pt to weights/yolov5l-cls.pt...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50.9M/50.9M [00:00<00:00, 73.9MB/s]\n\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-cls.pt to weights/yolov5x-cls.pt...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92.0M/92.0M [00:01<00:00, 66.7MB/s]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install roboflow","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:52:42.228011Z","iopub.execute_input":"2023-07-21T14:52:42.231528Z","iopub.status.idle":"2023-07-21T14:53:00.412673Z","shell.execute_reply.started":"2023-07-21T14:52:42.231449Z","shell.execute_reply":"2023-07-21T14:53:00.411450Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting roboflow\n  Downloading roboflow-1.1.2-py3-none-any.whl (57 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting certifi==2022.12.7 (from roboflow)\n  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting chardet==4.0.0 (from roboflow)\n  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting cycler==0.10.0 (from roboflow)\n  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\nCollecting idna==2.10 (from roboflow)\n  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.4.4)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from roboflow) (3.7.1)\nRequirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.23.5)\nRequirement already satisfied: opencv-python>=4.1.2 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.8.0.74)\nRequirement already satisfied: Pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from roboflow) (9.5.0)\nCollecting pyparsing==2.4.7 (from roboflow)\n  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.8.2)\nRequirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.0.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from roboflow) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.16.0)\nCollecting supervision (from roboflow)\n  Downloading supervision-0.11.1-py3-none-any.whl (55 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: urllib3>=1.26.6 in /opt/conda/lib/python3.10/site-packages (from roboflow) (1.26.15)\nCollecting wget (from roboflow)\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from roboflow) (4.65.0)\nRequirement already satisfied: PyYAML>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from roboflow) (6.0)\nRequirement already satisfied: requests-toolbelt in /opt/conda/lib/python3.10/site-packages (from roboflow) (0.10.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (1.1.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (4.40.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->roboflow) (21.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->roboflow) (3.1.0)\nBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=0f429da3291b6b0c30f851133931d5cb6ee53f3a883ba7554818fedc1f61a07f\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget, pyparsing, idna, cycler, chardet, certifi, supervision, roboflow\n  Attempting uninstall: pyparsing\n    Found existing installation: pyparsing 3.0.9\n    Uninstalling pyparsing-3.0.9:\n      Successfully uninstalled pyparsing-3.0.9\n  Attempting uninstall: idna\n    Found existing installation: idna 3.4\n    Uninstalling idna-3.4:\n      Successfully uninstalled idna-3.4\n  Attempting uninstall: cycler\n    Found existing installation: cycler 0.11.0\n    Uninstalling cycler-0.11.0:\n      Successfully uninstalled cycler-0.11.0\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2023.5.7\n    Uninstalling certifi-2023.5.7:\n      Successfully uninstalled certifi-2023.5.7\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab-lsp 4.2.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npytoolconfig 1.2.5 requires packaging>=22.0, but you have packaging 21.3 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed certifi-2022.12.7 chardet-4.0.0 cycler-0.10.0 idna-2.10 pyparsing-2.4.7 roboflow-1.1.2 supervision-0.11.1 wget-3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"%mkdir ../datasets/\n%cd ../datasets/","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:53:00.415933Z","iopub.execute_input":"2023-07-21T14:53:00.416625Z","iopub.status.idle":"2023-07-21T14:53:01.401731Z","shell.execute_reply.started":"2023-07-21T14:53:00.416584Z","shell.execute_reply":"2023-07-21T14:53:01.400249Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/working/datasets\n","output_type":"stream"}]},{"cell_type":"code","source":"from roboflow import Roboflow\nrf = Roboflow(api_key=\"RZLfGAnzm19he4Awf7kQ\")\nproject = rf.workspace(rf.current_workspace).project(\"oxford-flowers-102\")\ndataset = project.version(1).download(\"folder\")","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:53:01.404251Z","iopub.execute_input":"2023-07-21T14:53:01.404668Z","iopub.status.idle":"2023-07-21T14:53:16.271537Z","shell.execute_reply.started":"2023-07-21T14:53:01.404627Z","shell.execute_reply":"2023-07-21T14:53:16.270559Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"loading Roboflow workspace...\nloading Roboflow project...\nDownloading Dataset Version Zip in oxford-flowers-102-1 to folder: 100% [83573729 / 83573729] bytes\n","output_type":"stream"},{"name":"stderr","text":"Extracting Dataset Version Zip to oxford-flowers-102-1 in folder:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8500/8500 [00:01<00:00, 4250.36it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"#Save the dataset name to the environment so we can use it in a system call later\nimport os\ndataset_name = dataset.location.split(os.sep)[-1]\nos.environ[\"DATASET_NAME\"] = dataset_name","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:53:16.273058Z","iopub.execute_input":"2023-07-21T14:53:16.274047Z","iopub.status.idle":"2023-07-21T14:53:16.279259Z","shell.execute_reply.started":"2023-07-21T14:53:16.274010Z","shell.execute_reply":"2023-07-21T14:53:16.278097Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.login()","metadata":{"execution":{"iopub.status.busy":"2023-07-21T14:53:16.280879Z","iopub.execute_input":"2023-07-21T14:53:16.281810Z","iopub.status.idle":"2023-07-21T15:04:34.013425Z","shell.execute_reply.started":"2023-07-21T14:53:16.281777Z","shell.execute_reply":"2023-07-21T15:04:34.012395Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# yolov5s\n%cd ../yolov5\n!python classify/train.py --model yolov5s-cls.pt --data $DATASET_NAME --epochs 200 --pretrained weights/yolov5s-cls.pt","metadata":{"execution":{"iopub.status.busy":"2023-07-21T15:49:59.038464Z","iopub.execute_input":"2023-07-21T15:49:59.039468Z","iopub.status.idle":"2023-07-21T16:51:11.243882Z","shell.execute_reply.started":"2023-07-21T15:49:59.039426Z","shell.execute_reply":"2023-07-21T16:51:11.242363Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nWARNING âš ï¸ 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\nWARNING âš ï¸ 'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\nNote this warning may be related to loading older models. You can update your model to current structure with:\n    import torch\n    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n    torch.save(ckpt, \"updated-model.pt\")\n\n\u001b[34m\u001b[1mwandb\u001b[0m: WARNING âš ï¸ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomsega\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5s-cls.pt, data=oxford-flowers-102-1, epochs=200, batch_size=64, imgsz=224, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5s-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\nYOLOv5 ðŸš€ v7.0-193-g485da42 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/yolov5/wandb/run-20230721_155010-9tqulbcu\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msmart-serenity-7\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/tomsega/YOLOv5-Classify\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tomsega/YOLOv5-Classify/runs/9tqulbcu\u001b[0m\n\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\nModel summary: 149 layers, 4303142 parameters, 4303142 gradients, 10.6 GFLOPs\n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\nImage sizes 224 train, 224 test\nUsing 1 dataloader workers\nLogging results to \u001b[1mruns/train-cls/exp3\u001b[0m\nStarting yolov5s-cls.pt training on oxford-flowers-102-1 dataset with 102 classes for 200 epochs...\n\n     Epoch   GPU_mem  train_loss   test_loss    top1_acc    top5_acc\n     1/200     1.63G        4.25         4.1      0.0754       0.267: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     2/200     1.63G        3.96        3.93       0.096       0.343: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     3/200     1.63G        3.87        4.26      0.0764       0.277: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     4/200     1.63G        3.79        3.96      0.0945       0.339: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     5/200     1.63G        3.68        4.15      0.0896       0.293: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     6/200     1.63G        3.63        3.91       0.133       0.362: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     7/200     1.63G         3.6        3.86       0.126       0.374: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     8/200     1.63G        3.51        3.77       0.167       0.427: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     9/200     1.63G         3.5        3.75       0.162        0.43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    10/200     1.63G        3.46        3.48       0.175         0.5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    11/200     1.63G        3.44        3.37        0.21       0.541: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    12/200     1.63G         3.4        3.53       0.209       0.524: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    13/200     1.63G        3.36        3.41       0.211       0.529: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    14/200     1.63G        3.33        3.36        0.22       0.552: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    15/200     1.63G        3.27        3.26       0.266       0.576: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    16/200     1.63G        3.25        3.41       0.221       0.547: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    17/200     1.63G        3.26        3.29       0.245       0.562: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    18/200     1.63G        3.23        3.36       0.259       0.545: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    19/200     1.63G        3.21        3.29       0.261       0.576: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    20/200     1.63G        3.18        3.08       0.313       0.619: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    21/200     1.63G        3.12        3.11       0.277       0.626: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    22/200     1.63G        3.05        3.21       0.287       0.602: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    23/200     1.63G        3.02        3.17       0.305       0.614: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    24/200     1.63G        2.98        2.94       0.355       0.677: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    25/200     1.63G        2.97        2.87       0.364       0.695: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    26/200     1.63G        2.95        2.97       0.353        0.67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    27/200     1.63G        2.96        3.02       0.347       0.674: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    28/200     1.63G        2.85        3.13       0.312       0.635: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    29/200     1.63G        2.82         3.1       0.335       0.646: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    30/200     1.63G        2.81        2.88       0.386       0.701: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    31/200     1.63G        2.81        2.74       0.415       0.735: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    32/200     1.63G        2.72        2.84       0.391       0.718: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    33/200     1.63G        2.69        2.79       0.407       0.714: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    34/200     1.63G        2.67        2.69       0.431        0.74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    35/200     1.63G        2.64         2.7       0.445       0.743: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    36/200     1.63G        2.59        2.63       0.453       0.765: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    37/200     1.63G        2.57        2.51       0.484       0.772: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    38/200     1.63G        2.52        2.52       0.479       0.784: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    39/200     1.63G        2.48        2.58        0.48       0.754: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    40/200     1.63G        2.45        2.54       0.498       0.783: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    41/200     1.63G        2.43        2.54       0.467       0.771: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    42/200     1.63G        2.43        2.45       0.517       0.788: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    43/200     1.63G        2.39        2.46       0.512       0.789: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    44/200     1.63G        2.34         2.4       0.531       0.824: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    45/200     1.63G        2.32         2.4       0.538       0.807: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    46/200     1.63G        2.28        2.36        0.54       0.809: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    47/200     1.63G        2.25        2.28       0.563       0.834: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    48/200     1.63G        2.24        2.31       0.561       0.827: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    49/200     1.63G        2.25        2.25       0.576       0.833: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    50/200     1.63G         2.2        2.23       0.571       0.843: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    51/200     1.63G        2.18        2.18       0.604       0.853: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    52/200     1.63G        2.16        2.15       0.609       0.857: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    53/200     1.63G        2.11         2.2       0.589       0.847: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    54/200     1.63G        2.12        2.07       0.633       0.867: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    55/200     1.63G         2.1        2.11       0.622        0.87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    56/200     1.63G        2.09         2.1       0.613       0.863: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    57/200     1.63G        2.03        2.05       0.641       0.874: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    58/200     1.63G        2.02        2.01       0.652       0.881: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    59/200     1.63G        2.03        1.96       0.673       0.887: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    60/200     1.63G        1.94        2.01        0.65       0.888: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    61/200     1.63G        1.96        1.99       0.663       0.884: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    62/200     1.63G        1.93        1.96       0.672       0.892: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    63/200     1.63G        1.96        1.97        0.67       0.883: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    64/200     1.63G        1.94        1.95       0.674       0.888: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    65/200     1.63G        1.92        1.91       0.695       0.902: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    66/200     1.63G        1.87        1.87       0.705       0.903: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    67/200     1.63G        1.86        1.94       0.685       0.888: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    68/200     1.63G        1.85        1.88       0.692       0.896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    69/200     1.63G         1.8        1.86       0.716       0.897: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    70/200     1.63G        1.78        1.85        0.71       0.905: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    71/200     1.63G        1.82        1.88       0.693       0.902: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    72/200     1.63G        1.79        1.85        0.71       0.902: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    73/200     1.63G        1.77        1.81       0.725       0.909: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    74/200     1.63G        1.73        1.84       0.711       0.911: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    75/200     1.63G        1.77        1.78       0.733       0.918: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    76/200     1.63G        1.77         1.8       0.732       0.908: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    77/200     1.63G        1.72        1.77       0.739       0.919: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    78/200     1.63G        1.72         1.8       0.718       0.908: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    79/200     1.63G        1.68        1.77       0.739       0.914: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    80/200     1.63G        1.66        1.76       0.735       0.918: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    81/200     1.63G        1.68        1.72       0.759        0.92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    82/200     1.63G        1.68        1.75       0.743       0.917: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    83/200     1.63G        1.62        1.74       0.752       0.918: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    84/200     1.63G        1.62        1.71       0.758       0.927: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    85/200     1.63G        1.66        1.72       0.761       0.921: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    86/200     1.63G        1.64         1.7       0.762       0.927: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    87/200     1.63G        1.63        1.67       0.773       0.925: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    88/200     1.63G        1.62        1.67       0.776       0.927: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    89/200     1.63G        1.57         1.7       0.763       0.924: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    90/200     1.63G        1.61        1.66       0.776       0.928: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    91/200     1.63G        1.57        1.65       0.773       0.933: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    92/200     1.63G        1.57        1.65       0.774       0.927: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    93/200     1.63G        1.56        1.65       0.771       0.926: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    94/200     1.63G        1.55        1.65       0.778       0.932: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    95/200     1.63G        1.55        1.64       0.788       0.932: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    96/200     1.63G        1.55        1.62       0.782       0.932: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    97/200     1.63G        1.52        1.62       0.786       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    98/200     1.63G        1.49        1.64       0.791       0.934: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    99/200     1.63G         1.5        1.64       0.782       0.934: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   100/200     1.63G         1.5        1.61       0.793       0.934: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   101/200     1.63G        1.54        1.62       0.789        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   102/200     1.63G        1.49        1.61       0.787       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   103/200     1.63G        1.48         1.6        0.79       0.934: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   104/200     1.63G        1.46        1.59       0.798        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   105/200     1.63G        1.45        1.59         0.8        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   106/200     1.63G        1.48        1.58       0.795       0.938: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   107/200     1.63G        1.43        1.57       0.802       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   108/200     1.63G        1.45        1.56       0.815       0.939: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   109/200     1.63G        1.48        1.57       0.801       0.936: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   110/200     1.63G        1.45        1.55       0.809        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   111/200     1.63G        1.43        1.56       0.808        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   112/200     1.63G        1.41        1.55       0.805        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   113/200     1.63G        1.43        1.54       0.811       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   114/200     1.63G        1.39        1.55       0.809       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   115/200     1.63G        1.38        1.55        0.81       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   116/200     1.63G        1.39        1.52       0.819       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   117/200     1.63G        1.38        1.53       0.813       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   118/200     1.63G        1.37        1.53       0.809       0.944: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   119/200     1.63G        1.37        1.53       0.817       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   120/200     1.63G        1.41        1.53       0.815       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   121/200     1.63G        1.39        1.52       0.814       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   122/200     1.63G        1.37        1.52       0.813       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   123/200     1.63G        1.35        1.51       0.812       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   124/200     1.63G        1.37         1.5       0.822       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   125/200     1.63G        1.34        1.51       0.821       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   126/200     1.63G        1.36         1.5       0.828       0.949: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   127/200     1.63G        1.35         1.5       0.822       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   128/200     1.63G        1.31         1.5       0.824       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   129/200     1.63G        1.33         1.5       0.821        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   130/200     1.63G        1.35         1.5        0.82       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   131/200     1.63G        1.32        1.49       0.822        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   132/200     1.63G        1.33        1.49       0.825        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   133/200     1.63G        1.32         1.5       0.821       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   134/200     1.63G         1.3        1.49       0.825        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   135/200     1.63G         1.3        1.49       0.825       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   136/200     1.63G        1.28        1.49       0.826       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   137/200     1.63G         1.3        1.48       0.831       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   138/200     1.63G        1.29        1.49       0.825       0.949: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   139/200     1.63G        1.31        1.48       0.824       0.949: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   140/200     1.63G        1.26        1.49       0.826        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   141/200     1.63G        1.26        1.48       0.826        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   142/200     1.63G        1.27        1.48       0.827       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   143/200     1.63G        1.27        1.49       0.823       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   144/200     1.63G        1.26        1.48        0.83       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   145/200     1.63G        1.27        1.48        0.83       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   146/200     1.63G        1.24        1.47       0.831        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   147/200     1.63G        1.25        1.47        0.83       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   148/200     1.63G        1.25        1.47       0.834       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   149/200     1.63G        1.24        1.46       0.833       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   150/200     1.63G        1.24        1.47       0.829       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   151/200     1.63G        1.23        1.47       0.825       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   152/200     1.63G        1.26        1.47        0.83       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   153/200     1.63G        1.26        1.46       0.834       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   154/200     1.63G        1.21        1.46       0.834       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   155/200     1.63G        1.21        1.46       0.834       0.953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   156/200     1.63G         1.2        1.45       0.833       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   157/200     1.63G        1.21        1.45       0.837       0.953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   158/200     1.63G        1.21        1.45       0.833       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   159/200     1.63G         1.2        1.45       0.833       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   160/200     1.63G         1.2        1.45       0.836       0.955: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   161/200     1.63G         1.2        1.45       0.833       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   162/200     1.63G        1.18        1.44       0.833       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   163/200     1.63G         1.2        1.44       0.837       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   164/200     1.63G        1.21        1.44       0.841       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   165/200     1.63G        1.18        1.44       0.836       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   166/200     1.63G        1.19        1.44       0.838       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   167/200     1.63G        1.17        1.44       0.838       0.955: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   168/200     1.63G        1.16        1.44       0.839       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   169/200     1.63G        1.17        1.44       0.838       0.953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   170/200     1.63G        1.18        1.44       0.838       0.955: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   171/200     1.63G        1.16        1.44       0.837       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   172/200     1.63G        1.15        1.44       0.835       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   173/200     1.63G        1.17        1.44       0.838       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   174/200     1.63G        1.17        1.44       0.835       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   175/200     1.63G        1.15        1.44       0.836       0.955: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   176/200     1.63G        1.15        1.44       0.837       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   177/200     1.63G        1.15        1.44       0.836       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   178/200     1.63G        1.14        1.44       0.835       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   179/200     1.63G        1.12        1.44       0.836       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   180/200     1.63G        1.14        1.44       0.834       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   181/200     1.63G        1.15        1.44       0.834       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   182/200     1.63G        1.15        1.44       0.835       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   183/200     1.63G        1.13        1.44       0.835       0.955: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   184/200     1.63G        1.13        1.44       0.833       0.955: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   185/200     1.63G        1.13        1.44       0.832       0.955: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   186/200     1.63G        1.12        1.44       0.831       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   194/200     1.63G         1.1        1.44       0.833       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   195/200     1.63G         1.1        1.44       0.833       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   196/200     1.63G        1.11        1.44       0.833       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   197/200     1.63G        1.11        1.44       0.833       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   198/200     1.63G        1.09        1.44       0.833       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   199/200     1.63G        1.09        1.44       0.834       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   200/200     1.63G        1.09        1.44       0.836       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n\nTraining complete (1.002 hours)\nResults saved to \u001b[1mruns/train-cls/exp3\u001b[0m\nPredict:         python classify/predict.py --weights runs/train-cls/exp3/weights/best.pt --source im.jpg\nValidate:        python classify/val.py --weights runs/train-cls/exp3/weights/best.pt --data /kaggle/working/datasets/oxford-flowers-102-1\nExport:          python export.py --weights runs/train-cls/exp3/weights/best.pt --include onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp3/weights/best.pt')\nVisualize:       https://netron.app\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# yolov5n\n%cd ../yolov5\n!python classify/train.py --model yolov5n-cls.pt --data $DATASET_NAME --epochs 200 --pretrained weights/yolov5n-cls.pt","metadata":{"execution":{"iopub.status.busy":"2023-07-21T16:51:11.246497Z","iopub.execute_input":"2023-07-21T16:51:11.246819Z","iopub.status.idle":"2023-07-21T17:48:52.611971Z","shell.execute_reply.started":"2023-07-21T16:51:11.246790Z","shell.execute_reply":"2023-07-21T17:48:52.610342Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nWARNING âš ï¸ 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\nWARNING âš ï¸ 'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\nNote this warning may be related to loading older models. You can update your model to current structure with:\n    import torch\n    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n    torch.save(ckpt, \"updated-model.pt\")\n\n\u001b[34m\u001b[1mwandb\u001b[0m: WARNING âš ï¸ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomsega\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5n-cls.pt, data=oxford-flowers-102-1, epochs=200, batch_size=64, imgsz=224, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5s-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\nYOLOv5 ðŸš€ v7.0-193-g485da42 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/yolov5/wandb/run-20230721_165122-u19vrucv\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33molive-water-8\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/tomsega/YOLOv5-Classify\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tomsega/YOLOv5-Classify/runs/u19vrucv\u001b[0m\n\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-cls.pt to yolov5n-cls.pt...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.87M/4.87M [00:00<00:00, 58.3MB/s]\n\nModel summary: 149 layers, 1342662 parameters, 1342662 gradients, 3.1 GFLOPs\n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\nImage sizes 224 train, 224 test\nUsing 1 dataloader workers\nLogging results to \u001b[1mruns/train-cls/exp4\u001b[0m\nStarting yolov5n-cls.pt training on oxford-flowers-102-1 dataset with 102 classes for 200 epochs...\n\n     Epoch   GPU_mem  train_loss   test_loss    top1_acc    top5_acc\n     1/200    0.854G        4.32        4.53      0.0367       0.148: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     2/200    0.906G        3.98        4.02      0.0764       0.294: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     3/200    0.906G        3.84        3.83       0.107       0.355: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     4/200    0.906G        3.72        3.63       0.146       0.433: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     5/200    0.906G        3.65        3.89       0.129       0.363: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     6/200    0.906G        3.62        4.13      0.0926       0.322: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     7/200    0.906G        3.56        3.62       0.169       0.447: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     8/200    0.906G        3.45        3.43       0.215       0.516: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     9/200    0.906G         3.4        3.49       0.193       0.507: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    10/200    0.906G        3.36        3.45       0.214       0.509: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    11/200    0.906G        3.32        3.31       0.263       0.559: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    12/200    0.906G        3.26        3.33        0.25       0.551: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    13/200    0.906G        3.22        3.23       0.271       0.586: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    14/200    0.906G        3.12        3.27       0.267       0.604: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    15/200    0.906G        3.07        3.25       0.296       0.598: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    16/200    0.906G        3.05        3.13       0.333        0.62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    17/200    0.906G        3.01        3.08       0.326        0.64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    18/200    0.906G        2.98        3.02       0.353       0.665: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    19/200    0.906G        2.92        2.87       0.379       0.696: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    20/200    0.906G        2.86        3.02       0.347       0.662: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    21/200    0.906G        2.82        2.87       0.403       0.695: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    22/200    0.906G        2.74        2.91       0.379       0.688: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    23/200    0.906G        2.74        2.76       0.413       0.727: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    24/200    0.906G        2.66        2.88       0.425       0.724: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    25/200    0.906G        2.64        2.73       0.429       0.737: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    26/200    0.906G        2.59        2.69        0.44       0.739: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    27/200    0.906G        2.58        2.55       0.502       0.779: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    28/200    0.906G        2.49        2.55       0.498       0.784: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    29/200    0.906G        2.44        2.54       0.475       0.783: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    30/200    0.906G        2.46        2.66       0.465       0.749: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    31/200    0.906G        2.44         2.4       0.528       0.811: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    32/200    0.906G        2.33        2.34       0.553       0.825: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    33/200    0.906G        2.32        2.37       0.529       0.813: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    34/200    0.906G        2.29         2.4       0.537       0.817: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    35/200    0.906G        2.24        2.32        0.56       0.823: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    36/200    0.906G        2.21        2.39       0.529       0.814: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    37/200    0.906G        2.21        2.21        0.59       0.853: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    38/200    0.906G        2.14                             testing:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"    49/200    0.906G        1.95        1.97       0.674       0.892: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    50/200    0.906G         1.9        1.98       0.658       0.892: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    51/200    0.906G         1.9        2.01       0.667       0.887: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    52/200    0.906G        1.88        1.92       0.688       0.904: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    53/200    0.906G        1.84        1.92       0.685       0.896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    54/200    0.906G        1.83        1.89       0.673       0.905: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    55/200    0.906G        1.88        1.88        0.69       0.907: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    56/200    0.906G        1.85        1.85       0.707       0.913: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    57/200    0.906G        1.78        1.86         0.7       0.913: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    58/200    0.906G        1.73        1.85       0.703       0.915: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    59/200    0.906G        1.76        1.75        0.74       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    60/200    0.906G        1.73        1.82       0.706       0.917: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    61/200    0.906G        1.73         1.8       0.714       0.918: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    62/200    0.906G         1.7        1.73       0.755       0.931: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    63/200    0.906G        1.73        1.77       0.731        0.92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    64/200    0.906G         1.7        1.74       0.741       0.931: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    65/200    0.906G        1.65        1.72        0.75        0.93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    66/200    0.906G        1.65        1.68       0.758       0.936: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    67/200    0.906G        1.66        1.71       0.753       0.926: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    68/200    0.906G        1.63        1.72        0.76       0.928: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    69/200    0.906G         1.6        1.68       0.769       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    70/200    0.906G        1.59        1.68       0.759       0.939: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    71/200    0.906G        1.59        1.68       0.762       0.937: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    72/200    0.906G        1.58        1.69       0.752       0.938: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    73/200    0.906G        1.58        1.64       0.771        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    74/200    0.906G        1.53        1.65       0.771       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    75/200    0.906G        1.56        1.63       0.772       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    76/200    0.906G        1.55        1.63       0.777       0.936: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    77/200    0.906G        1.52         1.6       0.786       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    78/200    0.906G        1.52        1.64       0.782       0.944: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    79/200    0.906G        1.51        1.58       0.793       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    80/200    0.906G         1.5        1.58       0.798       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    81/200    0.906G         1.5        1.58         0.8       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    82/200    0.906G         1.5        1.59       0.797       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    83/200    0.906G        1.46        1.57       0.797       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    84/200    0.906G        1.45        1.56         0.8       0.949: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    85/200    0.906G        1.48        1.55       0.802        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    93/200    0.906G        1.41         1.5       0.813       0.955: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    94/200    0.906G        1.39        1.51       0.817       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    95/200    0.906G        1.41         1.5       0.821       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    96/200    0.906G         1.4        1.48       0.822       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    97/200    0.906G        1.38         1.5       0.816       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    98/200    0.906G        1.36        1.49       0.827       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    99/200    0.906G        1.35        1.49       0.822       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   100/200    0.906G        1.34        1.49       0.824       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   101/200    0.906G        1.36        1.47       0.829       0.959: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   102/200    0.906G        1.35        1.47       0.832       0.961: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   103/200    0.906G        1.35        1.46       0.834       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   104/200    0.906G         1.3        1.45        0.84       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   105/200    0.906G        1.32        1.45       0.837       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   106/200    0.906G        1.35        1.47       0.823       0.953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   107/200    0.906G        1.32        1.45       0.834       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   108/200    0.906G        1.32        1.43       0.843       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   109/200    0.906G        1.35        1.45       0.838       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   110/200    0.906G        1.33        1.44        0.84       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   111/200    0.906G         1.3        1.43       0.844       0.962: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   112/200    0.906G        1.29        1.44       0.833       0.961: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   113/200    0.906G        1.31        1.43       0.835       0.963: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   114/200    0.906G        1.29        1.43       0.839       0.965: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   115/200    0.906G        1.27        1.42       0.845       0.961: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   116/200    0.906G        1.26        1.41        0.85       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   117/200    0.906G        1.26        1.41       0.851       0.965: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   118/200    0.906G        1.25        1.42       0.847       0.962: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   119/200    0.906G        1.27        1.41       0.842        0.96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   120/200    0.906G         1.3        1.41        0.85       0.962: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   121/200    0.906G        1.26        1.42       0.849        0.96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   122/200    0.906G        1.26        1.41       0.852       0.965: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   123/200    0.906G        1.24         1.4       0.852       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   124/200    0.906G        1.26         1.4       0.854       0.965: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   125/200    0.906G        1.25        1.39       0.855       0.963: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   126/200    0.906G        1.24         1.4       0.857       0.964: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   127/200    0.906G        1.25         1.4       0.855       0.961: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   128/200    0.906G        1.22         1.4       0.855       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   129/200    0.906G        1.23        1.39       0.854       0.965: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   130/200    0.906G        1.22        1.39       0.852       0.965: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   131/200    0.906G        1.23        1.38       0.856       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   132/200    0.906G        1.25                                    :  17%|â–ˆâ–‹   ","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"   147/200    0.906G        1.18        1.36       0.864       0.965: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   148/200    0.906G        1.17        1.36       0.867       0.965: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   149/200    0.906G        1.17        1.36       0.864       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   150/200    0.906G        1.16        1.36       0.867       0.969: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   151/200    0.906G        1.17        1.36       0.865       0.969: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   152/200    0.906G        1.18        1.35       0.865       0.969: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   153/200    0.906G        1.17        1.35       0.867       0.969: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   154/200    0.906G        1.14        1.35       0.865       0.969: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   155/200    0.906G        1.14        1.35       0.867        0.97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   156/200    0.906G        1.13        1.35       0.865        0.97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   157/200    0.906G        1.14        1.34       0.865       0.968: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   158/200    0.906G        1.14        1.34       0.865        0.97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   159/200    0.906G        1.14        1.34       0.864       0.968: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   160/200    0.906G        1.13        1.34       0.865       0.969: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   161/200    0.906G        1.15        1.34       0.863        0.97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   162/200    0.906G        1.12        1.34       0.863       0.971: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   163/200    0.906G        1.14        1.34       0.867       0.969: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   164/200    0.906G        1.15        1.34       0.865       0.969: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   165/200    0.906G        1.12        1.35       0.866       0.968: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   166/200    0.906G        1.13        1.34       0.867       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   167/200    0.906G        1.12        1.34       0.868        0.97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   168/200    0.906G        1.11        1.34       0.869       0.971: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   169/200    0.906G        1.12        1.34       0.869       0.971: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   170/200    0.906G        1.11        1.34       0.868        0.97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   171/200    0.906G         1.1        1.34       0.868       0.968: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   172/200    0.906G        1.11        1.34       0.871       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   173/200    0.906G        1.11        1.34        0.87       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   174/200    0.906G        1.14        1.34       0.868       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   175/200    0.906G        1.09        1.34        0.87       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   176/200    0.906G         1.1        1.34       0.869       0.968: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   177/200    0.906G         1.1        1.34       0.866       0.969: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   178/200    0.906G         1.1        1.34       0.866       0.968: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   179/200    0.906G        1.08        1.34       0.865       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   180/200    0.906G         1.1        1.34       0.865       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   181/200    0.906G         1.1        1.34       0.865       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   182/200    0.906G        1.09        1.34       0.863       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   183/200    0.906G        1.09        1.34       0.864       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   184/200    0.906G        1.09        1.34       0.865       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   185/200    0.906G        1.08        1.34       0.866       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   186/200    0.906G        1.08        1.34       0.865       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   187/200    0.906G        1.07        1.34       0.865       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   188/200    0.906G        1.09        1.34       0.865       0.968: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   189/200    0.906G        1.09        1.33       0.866       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   190/200    0.906G         1.1        1.34       0.867       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   191/200    0.906G        1.08        1.33       0.869       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   192/200    0.906G        1.09        1.34       0.867       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   193/200    0.906G        1.05        1.34       0.867       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   194/200    0.906G        1.07        1.34       0.868       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   195/200    0.906G        1.07        1.34        0.87       0.966: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   196/200    0.906G        1.08        1.34       0.869       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   197/200    0.906G        1.07        1.34       0.868       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   198/200    0.906G        1.07        1.34       0.868       0.968: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   199/200    0.906G        1.05        1.34       0.868       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   200/200    0.906G        1.06        1.34       0.869       0.967: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n\nTraining complete (0.944 hours)\nResults saved to \u001b[1mruns/train-cls/exp4\u001b[0m\nPredict:         python classify/predict.py --weights runs/train-cls/exp4/weights/best.pt --source im.jpg\nValidate:        python classify/val.py --weights runs/train-cls/exp4/weights/best.pt --data /kaggle/working/datasets/oxford-flowers-102-1\nExport:          python export.py --weights runs/train-cls/exp4/weights/best.pt --include onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp4/weights/best.pt')\nVisualize:       https://netron.app\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# yolov5m\n%cd ../yolov5\n!python classify/train.py --model yolov5m-cls.pt --data $DATASET_NAME --epochs 200 --pretrained weights/yolov5m-cls.pt","metadata":{"execution":{"iopub.status.busy":"2023-07-21T17:48:52.613894Z","iopub.execute_input":"2023-07-21T17:48:52.614288Z","iopub.status.idle":"2023-07-21T18:56:02.847993Z","shell.execute_reply.started":"2023-07-21T17:48:52.614247Z","shell.execute_reply":"2023-07-21T18:56:02.846419Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nWARNING âš ï¸ 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\nWARNING âš ï¸ 'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\nNote this warning may be related to loading older models. You can update your model to current structure with:\n    import torch\n    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n    torch.save(ckpt, \"updated-model.pt\")\n\n\u001b[34m\u001b[1mwandb\u001b[0m: WARNING âš ï¸ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomsega\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5m-cls.pt, data=oxford-flowers-102-1, epochs=200, batch_size=64, imgsz=224, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5s-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\nYOLOv5 ðŸš€ v7.0-193-g485da42 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/yolov5/wandb/run-20230721_174904-36b46rrz\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mrevived-tree-9\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/tomsega/YOLOv5-Classify\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tomsega/YOLOv5-Classify/runs/36b46rrz\u001b[0m\n\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\nDownloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-cls.pt to yolov5m-cls.pt...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.9M/24.9M [00:00<00:00, 43.2MB/s]\n\nModel summary: 212 layers, 11807302 parameters, 11807302 gradients, 31.0 GFLOPs\n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 46 weight(decay=0.0), 47 weight(decay=5e-05), 47 bias\nImage sizes 224 train, 224 test\nUsing 1 dataloader workers\nLogging results to \u001b[1mruns/train-cls/exp5\u001b[0m\nStarting yolov5m-cls.pt training on oxford-flowers-102-1 dataset with 102 classes for 200 epochs...\n\n     Epoch   GPU_mem  train_loss   test_loss    top1_acc    top5_acc\n     1/200     2.78G        4.46        4.76      0.0215       0.131: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     2/200     2.78G        4.18        10.3      0.0274       0.142: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     3/200     2.78G        4.06        4.19      0.0891       0.265: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     4/200     2.78G        4.05        3.94      0.0926       0.306: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     5/200     2.78G        3.99           4      0.0911        0.28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     6/200     2.78G        3.96        3.99      0.0837       0.283: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     7/200     2.78G        3.91        4.36      0.0563       0.206: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     8/200     2.78G        3.84        4.18      0.0788       0.247: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     9/200     2.78G        3.79        3.95       0.109       0.335: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    10/200     2.78G        3.75        3.86       0.116       0.386: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    11/200     2.78G        3.71        4.25      0.0906       0.333: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    12/200     2.78G        3.65        3.82       0.116       0.386: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    13/200     2.78G        3.62        3.77       0.151       0.389: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    14/200     2.78G        3.56        3.81       0.163       0.438: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    15/200     2.78G        3.48        3.78       0.143         0.4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    16/200     2.78G        3.48        3.69       0.164       0.434: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    17/200     2.78G         3.5        3.53       0.199       0.495: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    18/200     2.78G        3.44        3.42       0.228       0.531: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    19/200     2.78G         3.4        3.46       0.218       0.524: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    20/200     2.78G        3.38        3.45       0.225       0.518: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    21/200     2.78G        3.33        3.35       0.253       0.559: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    22/200     2.78G        3.28        3.37       0.238        0.56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    23/200     2.78G        3.26        3.32       0.256       0.551: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    24/200     2.78G        3.22        3.31       0.253       0.564: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    25/200     2.78G        3.23        3.14       0.305       0.614: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    26/200     2.78G        3.18        3.16        0.29       0.621: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    27/200     2.78G        3.18        3.17       0.297       0.603: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    28/200     2.78G        3.08        3.18       0.297         0.6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    29/200     2.78G        3.08        3.33       0.253       0.566: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    30/200     2.78G        3.08        3.13       0.314       0.622: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    31/200     2.78G        3.09        3.07       0.333       0.637: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    32/200     2.78G        3.04        3.06       0.319       0.643: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    33/200     2.78G        2.98        3.16       0.301       0.603: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    34/200     2.78G        2.98        3.03       0.329       0.634: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    35/200     2.78G        2.93        3.06       0.342       0.633: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    36/200     2.78G         2.9        3.02       0.343       0.636: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    37/200     2.78G         2.9        2.98       0.358       0.667: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    38/200     2.78G        2.86        2.79       0.397       0.707: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    39/200     2.78G        2.83        2.81       0.405       0.706: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    40/200     2.78G         2.8        2.83       0.393       0.706: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    41/200     2.78G        2.75        2.83       0.387       0.704: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    42/200     2.78G        2.79        2.86       0.401       0.707: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    43/200     2.78G        2.71        2.71       0.428       0.728: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    44/200     2.78G        2.69        2.72       0.444       0.741: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    45/200     2.78G        2.67        2.65        0.46       0.754: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    46/200     2.78G        2.58        2.71       0.429       0.733: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    47/200     2.78G        2.62        2.62        0.46       0.762: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    48/200     2.78G        2.62        2.54        0.48       0.778: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    49/200     2.78G         2.6        2.58       0.462       0.763: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    50/200     2.78G        2.58        2.52       0.503        0.78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    51/200     2.78G        2.56        2.55        0.48       0.772: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    52/200     2.78G        2.49        2.63       0.446       0.749: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    53/200     2.78G        2.46        2.56       0.472       0.767: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    54/200     2.78G        2.51        2.48       0.501        0.79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    55/200     2.78G         2.5        2.54       0.483       0.779: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    56/200     2.78G        2.44        2.48       0.498       0.789: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    57/200     2.78G        2.42        2.43       0.513       0.804: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    58/200     2.78G        2.37        2.39       0.531        0.81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    59/200     2.78G        2.39        2.33       0.547       0.822: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    60/200     2.78G        2.33        2.35       0.539       0.815: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    61/200     2.78G        2.32        2.36       0.526       0.815: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    62/200     2.78G        2.29        2.29       0.561       0.824: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    63/200     2.78G        2.29        2.36       0.546       0.811: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    64/200     2.78G        2.26        2.23       0.572       0.849: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    65/200     2.78G        2.25        2.28       0.564       0.832: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    66/200     2.78G        2.21        2.21       0.585       0.846: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    67/200     2.78G         2.2        2.19       0.594       0.842: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    68/200     2.78G        2.19        2.19       0.588       0.852: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    69/200     2.78G        2.13        2.17       0.598       0.854: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    70/200     2.78G        2.11        2.18       0.594       0.861: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    71/200     2.78G        2.13        2.16       0.608       0.853: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    72/200     2.78G         2.1        2.13       0.617       0.856: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    73/200     2.78G        2.08         2.1       0.619       0.865: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    74/200     2.78G        2.03         2.1       0.625        0.87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    75/200     2.78G        2.08        2.09       0.632       0.877: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    76/200     2.78G        2.06        2.09       0.626       0.873: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    77/200     2.78G        2.04        2.02       0.659       0.881: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    78/200     2.78G        2.01        2.02       0.639       0.878: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    79/200     2.78G        1.96        2.02       0.651       0.881: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    80/200     2.78G        1.93        1.99       0.664       0.889: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    81/200     2.78G        1.94        1.99       0.655       0.889: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    82/200     2.78G        1.95        1.98       0.666       0.884: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    83/200     2.78G        1.89        1.94       0.671       0.897: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    84/200     2.78G        1.87        1.94        0.67       0.901: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    85/200     2.78G        1.89        1.94       0.671       0.892: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    86/200     2.78G        1.87        1.93       0.675       0.896: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    87/200     2.78G        1.86         1.9       0.687       0.909: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    88/200     2.78G        1.83         1.9       0.689       0.902: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    89/200     2.78G         1.8        1.88       0.698       0.908: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    90/200     2.78G        1.85        1.87       0.704       0.903: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    91/200     2.78G        1.82        1.87       0.704         0.9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    92/200     2.78G        1.79        1.84       0.715       0.914: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    93/200     2.78G        1.78        1.83        0.71       0.913: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    94/200     2.78G        1.75        1.83       0.712       0.909: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    95/200     2.78G        1.77        1.83       0.718       0.906: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    96/200     2.78G        1.73        1.81       0.718       0.914: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    97/200     2.78G        1.72         1.8       0.726       0.916: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    98/200     2.78G        1.71        1.81       0.727       0.914: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    99/200     2.78G         1.7        1.79       0.741       0.911: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   100/200     2.78G        1.69        1.79       0.723       0.913: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   101/200     2.78G         1.7        1.79       0.726       0.917: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   102/200     2.78G        1.66        1.78       0.731       0.914: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   103/200     2.78G        1.66        1.75       0.739        0.92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   104/200     2.78G        1.62        1.74       0.754       0.915: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   105/200     2.78G        1.58        1.74       0.746       0.923: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   106/200     2.78G        1.66        1.74       0.741       0.918: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   107/200     2.78G        1.62        1.73       0.747       0.919: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   108/200     2.78G        1.61        1.74       0.745       0.917: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   109/200     2.78G        1.62        1.73       0.756       0.921: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   110/200     2.78G        1.62        1.71       0.774       0.928: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   111/200     2.78G        1.58         1.7       0.753       0.925: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   112/200     2.78G        1.56        1.68       0.762       0.931: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   113/200     2.78G        1.56        1.69       0.766       0.931: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   114/200     2.78G        1.54        1.69       0.771       0.931: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   115/200     2.78G        1.52        1.67       0.771       0.929: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   116/200     2.78G        1.52        1.67       0.766       0.926: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   117/200     2.78G        1.51        1.67       0.769       0.932: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   118/200     2.78G         1.5        1.67       0.768       0.929: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   119/200     2.78G        1.51        1.67       0.769       0.929: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   120/200     2.78G        1.52        1.67       0.767       0.924: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   121/200     2.78G        1.49        1.66       0.775        0.93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   122/200     2.78G        1.48        1.66       0.773       0.932: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   123/200     2.78G        1.47        1.64       0.773        0.93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   124/200     2.78G        1.49        1.64       0.775       0.938: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   125/200     2.78G        1.46        1.64       0.774       0.932: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   126/200     2.78G        1.44        1.62       0.791       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   127/200     2.78G        1.44        1.62       0.793       0.936: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   128/200     2.78G        1.41        1.62       0.779       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   129/200     2.78G        1.42        1.61       0.794       0.932: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   130/200     2.78G        1.42        1.61       0.789       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   131/200     2.78G        1.43         1.6       0.781       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   132/200     2.78G        1.44        1.61       0.789       0.936: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   133/200     2.78G         1.4         1.6       0.793       0.936: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   134/200     2.78G        1.37        1.59         0.8        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   135/200     2.78G        1.37        1.59       0.792       0.938: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   136/200     2.78G        1.34        1.59       0.791        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   137/200     2.78G        1.37        1.58       0.792       0.939: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   138/200     2.78G        1.34        1.58       0.796       0.936: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   139/200     2.78G        1.36        1.58       0.793       0.936: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   140/200     2.78G        1.33        1.58       0.796       0.937: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   141/200     2.78G        1.33        1.58       0.794       0.937: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   142/200     2.78G        1.34        1.58       0.789       0.936: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   143/200     2.78G        1.34        1.59        0.79       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   144/200     2.78G        1.33        1.58       0.797       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   145/200     2.78G        1.33        1.57         0.8       0.939: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   146/200     2.78G         1.3        1.56       0.799       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   147/200     2.78G        1.33        1.56       0.804       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   148/200     2.78G        1.32        1.55       0.807        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   149/200     2.78G        1.29        1.55        0.81       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   150/200     2.78G        1.28        1.55       0.807       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   151/200     2.78G        1.29        1.55       0.811       0.944: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   152/200     2.78G        1.31        1.55        0.81       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   153/200     2.78G        1.31        1.54       0.809       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   154/200     2.78G        1.26        1.54       0.806       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   155/200     2.78G        1.28        1.54       0.805       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   156/200     2.78G        1.24        1.54       0.809       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   157/200     2.78G        1.25        1.53        0.81       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   158/200     2.78G        1.25        1.53       0.811       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   159/200     2.78G        1.25        1.53       0.814       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   160/200     2.78G        1.23        1.52       0.817       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   161/200     2.78G        1.24        1.52       0.815       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   162/200     2.78G        1.22        1.51        0.82       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   163/200     2.78G        1.23        1.51        0.82       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   164/200     2.78G        1.24        1.51       0.823       0.944: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   165/200     2.78G        1.21        1.51       0.823       0.944: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   166/200     2.78G        1.23         1.5       0.826       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   167/200     2.78G        1.19         1.5       0.826       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   168/200     2.78G        1.19         1.5       0.826       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   169/200     2.78G        1.19         1.5       0.827       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   170/200     2.78G        1.21         1.5       0.827       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   171/200     2.78G        1.19         1.5       0.825       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   172/200     2.78G        1.18         1.5       0.823       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   173/200     2.78G        1.19         1.5       0.825       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   174/200     2.78G        1.19         1.5       0.827       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   175/200     2.78G        1.19         1.5       0.826       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   176/200     2.78G        1.17         1.5       0.826       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   177/200     2.78G        1.18        1.49       0.824       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   178/200     2.78G        1.17        1.49       0.821       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   179/200     2.78G        1.14        1.49       0.822       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   180/200     2.78G        1.15        1.49        0.82       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   181/200     2.78G        1.17        1.49       0.821       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   188/200     2.78G        1.14        1.49       0.827       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   189/200     2.78G        1.13        1.49       0.827       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   190/200     2.78G        1.16        1.49       0.829       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   191/200     2.78G        1.12        1.49       0.828       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   192/200     2.78G        1.15        1.49       0.827       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   193/200     2.78G         1.1        1.49       0.827       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   194/200     2.78G        1.11        1.49       0.826       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   195/200     2.78G        1.12        1.49       0.826       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   196/200     2.78G        1.12        1.49       0.826       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   197/200     2.78G        1.11        1.49       0.824       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   198/200     2.78G        1.11        1.49       0.823       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   199/200     2.78G        1.09        1.49       0.824       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   200/200     2.78G         1.1        1.49       0.825       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n\nTraining complete (1.102 hours)\nResults saved to \u001b[1mruns/train-cls/exp5\u001b[0m\nPredict:         python classify/predict.py --weights runs/train-cls/exp5/weights/best.pt --source im.jpg\nValidate:        python classify/val.py --weights runs/train-cls/exp5/weights/best.pt --data /kaggle/working/datasets/oxford-flowers-102-1\nExport:          python export.py --weights runs/train-cls/exp5/weights/best.pt --include onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp5/weights/best.pt')\nVisualize:       https://netron.app\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-07-21T18:56:02.851713Z","iopub.execute_input":"2023-07-21T18:56:02.852509Z","iopub.status.idle":"2023-07-21T18:56:37.071276Z","shell.execute_reply.started":"2023-07-21T18:56:02.852448Z","shell.execute_reply":"2023-07-21T18:56:37.070189Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Exporting format folder in progress : 85.0%\nVersion export complete for folder format\nDownloading Dataset Version Zip in oxford-flowers-102-2 to folder: 29% [24453120 / 83547915] bytes","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"Downloading Dataset Version Zip in oxford-flowers-102-2 to folder: 66% [55746560 / 83547915] bytes","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"Downloading Dataset Version Zip in oxford-flowers-102-2 to folder: 100% [83547915 / 83547915] bytes\n","output_type":"stream"},{"name":"stderr","text":"Extracting Dataset Version Zip to oxford-flowers-102-2 in folder:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8300/8300 [00:01<00:00, 7267.03it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# shuffle data splits\nimport glob\nimport random\nimport os\nimport shutil\ntrain_paths = glob.glob(\"/kaggle/working/datasets/oxford-flowers-102-1/train/*/*\")\nval_paths = glob.glob(\"/kaggle/working/datasets/oxford-flowers-102-1/valid/*/*\")\ntest_paths = glob.glob(\"/kaggle/working/datasets/oxford-flowers-102-1/test/*/*\")\nall_paths = train_paths + val_paths + test_paths\nds_size = len(all_paths)\nall_inds = list(range(ds_size))\nval_test_inds = set(random.sample(all_inds, int(0.5*ds_size)))\ntest_inds = set(random.sample(list(val_test_inds), int(0.25*ds_size)))\n\ndef my_mkdir(path):\n    if not os.path.exists(path):\n        os.mkdir(path)\n        \ndata_folder = \"/kaggle/working/datasets/oxford-flowers-102-2\"\nmy_mkdir(data_folder)\nfor split in [\"train\",\"valid\",\"test\"]:\n    my_mkdir(f\"{data_folder}/{split}\")\n    for label in range(1,103):\n        my_mkdir(f\"{data_folder}/{split}/{label}\")\n\nfor i,path in enumerate(all_paths):\n    label, fn = path.split(\"/\")[-2:]\n    if i in test_inds:\n        new_path = f\"{data_folder}/test/{label}/{fn}\"\n    elif i in val_test_inds:\n        new_path = f\"{data_folder}/valid/{label}/{fn}\"\n    else:\n        new_path = f\"{data_folder}/train/{label}/{fn}\"\n    shutil.copy(path, new_path)","metadata":{"execution":{"iopub.status.busy":"2023-07-21T20:01:52.168261Z","iopub.execute_input":"2023-07-21T20:01:52.168687Z","iopub.status.idle":"2023-07-21T20:01:53.125022Z","shell.execute_reply.started":"2023-07-21T20:01:52.168652Z","shell.execute_reply":"2023-07-21T20:01:53.123992Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Sampling from a set deprecated\nsince Python 3.9 and will be removed in a subsequent version.\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ[\"DATASET_NAME2\"] = data_folder.split(\"/\")[-1]","metadata":{"execution":{"iopub.status.busy":"2023-07-21T20:07:28.615921Z","iopub.execute_input":"2023-07-21T20:07:28.616355Z","iopub.status.idle":"2023-07-21T20:07:28.622762Z","shell.execute_reply.started":"2023-07-21T20:07:28.616322Z","shell.execute_reply":"2023-07-21T20:07:28.621571Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# yolov5n\n%cd ../yolov5\n!python classify/train.py --model yolov5n-cls.pt --data $DATASET_NAME2 --epochs 200 --pretrained weights/yolov5n-cls.pt","metadata":{"execution":{"iopub.status.busy":"2023-07-21T20:08:07.867301Z","iopub.execute_input":"2023-07-21T20:08:07.868514Z","iopub.status.idle":"2023-07-21T21:05:27.140255Z","shell.execute_reply.started":"2023-07-21T20:08:07.868453Z","shell.execute_reply":"2023-07-21T21:05:27.138820Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"/kaggle/working/yolov5\n/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\nWARNING âš ï¸ 'ultralytics.yolo.v8' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.models.yolo' instead.\nWARNING âš ï¸ 'ultralytics.yolo.utils' is deprecated since '8.0.136' and will be removed in '8.1.0'. Please use 'ultralytics.utils' instead.\nNote this warning may be related to loading older models. You can update your model to current structure with:\n    import torch\n    ckpt = torch.load(\"model.pt\")  # applies to both official and custom models\n    torch.save(ckpt, \"updated-model.pt\")\n\n\u001b[34m\u001b[1mwandb\u001b[0m: WARNING âš ï¸ wandb is deprecated and will be removed in a future release. See supported integrations at https://github.com/ultralytics/yolov5#integrations.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtomsega\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5n-cls.pt, data=oxford-flowers-102-2, epochs=200, batch_size=64, imgsz=224, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5n-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 âœ…\nYOLOv5 ðŸš€ v7.0-193-g485da42 Python-3.10.12 torch-2.0.0 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/yolov5/wandb/run-20230721_200819-aciglel6\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfresh-feather-12\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/tomsega/YOLOv5-Classify\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tomsega/YOLOv5-Classify/runs/aciglel6\u001b[0m\n\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\nModel summary: 149 layers, 1342662 parameters, 1342662 gradients, 3.1 GFLOPs\n\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\nImage sizes 224 train, 224 test\nUsing 1 dataloader workers\nLogging results to \u001b[1mruns/train-cls/exp8\u001b[0m\nStarting yolov5n-cls.pt training on oxford-flowers-102-2 dataset with 102 classes for 200 epochs...\n\n     Epoch   GPU_mem  train_loss   test_loss    top1_acc    top5_acc\n     1/200    0.839G        4.24         4.5      0.0298       0.148: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     2/200    0.891G        3.91        3.83       0.129       0.357: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     3/200    0.891G        3.78        3.83       0.112       0.365: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     4/200    0.891G        3.69        3.82       0.134       0.384: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     5/200    0.891G         3.6        3.64       0.165       0.437: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     6/200    0.891G        3.53        3.63       0.164        0.47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     7/200    0.891G        3.44        3.64       0.196       0.461: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     8/200    0.891G        3.39        3.46       0.214       0.511: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n     9/200    0.891G        3.36        3.53       0.214       0.513: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    10/200    0.891G        3.28        3.36       0.254       0.564: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    11/200    0.891G        3.24        3.28       0.277       0.582: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    12/200    0.891G        3.16        3.23       0.314       0.619: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    13/200    0.891G        3.06         3.2       0.314       0.606: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    14/200    0.891G        3.02        3.19       0.288        0.62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    15/200    0.891G        2.97        3.06       0.333       0.633: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    16/200    0.891G        2.96        2.92       0.377       0.677: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    17/200    0.891G        2.89        3.45       0.302       0.597: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    18/200    0.891G        2.81         2.8       0.405       0.713: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    19/200    0.891G        2.75        3.03       0.371       0.665: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    20/200    0.891G        2.72        2.76       0.435       0.733: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    21/200    0.891G        2.67        2.72       0.444       0.739: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    22/200    0.891G        2.59        2.64       0.447       0.753: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    23/200    0.891G        2.56        2.86       0.422       0.716: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    24/200    0.891G        2.53        2.82       0.416        0.72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    25/200    0.891G        2.51        2.45       0.527       0.802: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    26/200    0.891G        2.45        2.42       0.513       0.809: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    27/200    0.891G        2.39        2.56       0.488       0.779: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    28/200    0.891G        2.33        2.49       0.507       0.798: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    29/200    0.891G        2.28        2.43       0.523         0.8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    30/200    0.891G        2.26        2.38       0.538       0.817: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    31/200    0.891G        2.23        2.26       0.576       0.839: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    32/200    0.891G        2.19         2.3       0.556       0.832: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    33/200    0.891G        2.17        2.33       0.562       0.829: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    34/200    0.891G        2.15        2.23       0.589       0.847: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    35/200    0.891G         2.1        2.19       0.593       0.852: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    36/200    0.891G        2.07        2.16        0.61        0.85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    37/200    0.891G        2.06        2.09       0.629       0.868: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    38/200    0.891G        2.04        2.12       0.612       0.864: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    39/200    0.891G        1.98        2.09       0.632       0.872: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    40/200    0.891G        1.95         2.1       0.625        0.87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    41/200    0.891G        1.95        2.08       0.626       0.867: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    42/200    0.891G        1.96        2.23       0.591       0.843: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    43/200    0.891G         1.9        1.99       0.658        0.88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    44/200    0.891G        1.89        2.02       0.654       0.879: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    45/200    0.891G        1.87        2.01       0.659       0.884: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    46/200    0.891G        1.85        2.01       0.669       0.878: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    47/200    0.891G        1.86        1.94       0.684        0.89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    48/200    0.891G        1.81         1.9        0.69        0.89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    49/200    0.891G        1.79        1.93       0.692       0.886: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    50/200    0.891G        1.78        1.96       0.673       0.875: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    51/200    0.891G        1.77        1.95       0.682       0.891: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    52/200    0.891G        1.74        1.89       0.699       0.897: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    53/200    0.891G        1.73        1.79       0.723       0.914: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    54/200    0.891G        1.69        1.83       0.726        0.91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    55/200    0.891G        1.68        1.85       0.715       0.901: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    56/200    0.891G        1.68        1.92       0.686         0.9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    57/200    0.891G        1.67        1.83       0.722       0.904: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    58/200    0.891G        1.64        1.77       0.733       0.913: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    59/200    0.891G        1.63        1.77       0.742       0.909: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    60/200    0.891G        1.62        1.83       0.723       0.901: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    61/200    0.891G         1.6        1.81       0.721       0.907: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    62/200    0.891G        1.59        1.77       0.748       0.905: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    63/200    0.891G        1.58        1.75       0.749       0.909: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    64/200    0.891G        1.56        1.75       0.752       0.916: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    65/200    0.891G        1.59        1.72       0.756       0.923: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    66/200    0.891G        1.56        1.73       0.764       0.915: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    67/200    0.891G        1.55         1.7       0.766       0.927: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    68/200    0.891G        1.52        1.71       0.766        0.92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    69/200    0.891G        1.54        1.74       0.745       0.915: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    70/200    0.891G        1.49        1.69       0.772       0.922: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    71/200    0.891G         1.5        1.69       0.766       0.916: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    72/200    0.891G         1.5        1.68       0.767       0.919: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    73/200    0.891G        1.49        1.69       0.757       0.925: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    74/200    0.891G        1.47        1.67        0.78       0.925: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    75/200    0.891G        1.46        1.67       0.766       0.925: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    76/200    0.891G        1.46        1.62       0.779        0.93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    77/200    0.891G        1.46        1.61       0.785       0.924: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    78/200    0.891G        1.44        1.64       0.775       0.929: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    79/200    0.891G        1.44        1.62       0.787       0.928: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    80/200    0.891G        1.42        1.61       0.791       0.931: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    81/200    0.891G         1.4         1.6       0.777       0.934: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    82/200    0.891G        1.41        1.62       0.783       0.931: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    83/200    0.891G        1.39        1.61       0.787       0.928: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    84/200    0.891G        1.36         1.6       0.795       0.929: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    85/200    0.891G        1.39        1.58       0.794       0.937: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    86/200    0.891G         1.4        1.58       0.799       0.932: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    87/200    0.891G        1.38        1.58       0.794       0.934: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    88/200    0.891G        1.35        1.58       0.797        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    89/200    0.891G        1.36        1.56       0.802       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    90/200    0.891G        1.33        1.56        0.81       0.929: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    91/200    0.891G        1.35        1.56       0.802       0.934: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    92/200    0.891G        1.34        1.54       0.807       0.938: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    93/200    0.891G        1.33        1.56       0.805       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    94/200    0.891G        1.33        1.56       0.802       0.935: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    95/200    0.891G         1.3        1.54       0.811       0.934: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    96/200    0.891G         1.3        1.53       0.806       0.936: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    97/200    0.891G        1.28        1.52       0.815       0.937: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    98/200    0.891G        1.31        1.51       0.817       0.939: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n    99/200    0.891G        1.28        1.52       0.812       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   100/200    0.891G        1.28        1.51       0.811       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   101/200    0.891G        1.29        1.52       0.812       0.939: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   102/200    0.891G        1.29        1.51       0.819       0.939: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   103/200    0.891G        1.25         1.5       0.827       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   104/200    0.891G        1.26         1.5       0.823       0.939: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   105/200    0.891G        1.27        1.52       0.819        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   106/200    0.891G        1.27        1.49       0.825       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   107/200    0.891G        1.24         1.5       0.822       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   108/200    0.891G        1.23        1.49       0.827       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   109/200    0.891G        1.24        1.48       0.826       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   110/200    0.891G        1.25        1.49        0.82        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   111/200    0.891G        1.25        1.49       0.823        0.94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   112/200    0.891G        1.22        1.48       0.828       0.944: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   113/200    0.891G        1.23        1.48       0.821       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   114/200    0.891G        1.21        1.48       0.827       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   115/200    0.891G        1.23        1.48       0.825       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   116/200    0.891G        1.21        1.47       0.833       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   117/200    0.891G        1.21        1.48       0.834       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   118/200    0.891G        1.21        1.47       0.834       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   119/200    0.891G         1.2        1.46       0.834       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   120/200    0.891G        1.21        1.46       0.839       0.942: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   121/200    0.891G        1.21        1.46       0.828       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   122/200    0.891G        1.22        1.46       0.828       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   123/200    0.891G        1.19        1.45       0.831       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   124/200    0.891G        1.19        1.45       0.831       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   125/200    0.891G        1.18        1.45       0.834       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   126/200    0.891G        1.18        1.45       0.838       0.943: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   127/200    0.891G        1.18        1.44       0.836       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   128/200    0.891G        1.18        1.45       0.837       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   129/200    0.891G        1.17        1.45       0.839       0.946: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   130/200    0.891G        1.19        1.45       0.833       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   131/200    0.891G        1.15        1.45       0.832       0.941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   132/200    0.891G        1.16        1.44        0.83       0.945: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   133/200    0.891G        1.17        1.44       0.831       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   134/200    0.891G        1.16        1.44       0.836       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   135/200    0.891G        1.14        1.43       0.837       0.947: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   136/200    0.891G        1.14        1.43       0.834       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   137/200    0.891G        1.15        1.43       0.838       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   138/200    0.891G        1.14        1.43       0.835       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   139/200    0.891G        1.14        1.43       0.836       0.949: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   140/200    0.891G        1.12        1.42       0.837       0.949: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   141/200    0.891G        1.15        1.42       0.834        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   142/200    0.891G        1.14        1.42       0.842       0.953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   143/200    0.891G        1.13        1.42        0.84       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   144/200    0.891G        1.13        1.42       0.841        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   145/200    0.891G        1.13        1.42       0.839       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   146/200    0.891G        1.12        1.42       0.844       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   147/200    0.891G        1.12        1.42       0.843       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   148/200    0.891G         1.1        1.42        0.84       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   149/200    0.891G        1.13        1.42       0.841       0.949: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   150/200    0.891G        1.11        1.42       0.844       0.949: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   151/200    0.891G        1.13        1.42       0.837       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   152/200    0.891G         1.1        1.41       0.842       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   153/200    0.891G         1.1        1.41       0.844       0.948: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   154/200    0.891G        1.11        1.41       0.842        0.95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   155/200    0.891G        1.09        1.41       0.838       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   156/200    0.891G         1.1        1.41       0.841       0.953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   157/200    0.891G         1.1        1.41       0.846       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   158/200    0.891G        1.08        1.41       0.849       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   159/200    0.891G         1.1        1.41       0.845       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   160/200    0.891G        1.08        1.41       0.842       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   161/200    0.891G         1.1        1.41       0.843       0.951: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   162/200    0.891G        1.07        1.41       0.845       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   163/200    0.891G        1.09         1.4       0.848       0.953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   164/200    0.891G        1.08         1.4       0.843       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   165/200    0.891G        1.07         1.4       0.844       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   166/200    0.891G        1.08         1.4        0.85       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   167/200    0.891G        1.08         1.4       0.845       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   168/200    0.891G        1.08         1.4       0.846       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   169/200    0.891G        1.06         1.4       0.847       0.955: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   170/200    0.891G        1.07         1.4       0.848       0.955: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   171/200    0.891G        1.07         1.4       0.844       0.953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   172/200    0.891G        1.06         1.4       0.846       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   173/200    0.891G        1.06         1.4       0.848       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   174/200    0.891G        1.06         1.4       0.847       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   175/200    0.891G        1.06         1.4       0.848       0.954: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   176/200    0.891G        1.06        1.39       0.848       0.953: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   177/200    0.891G        1.05        1.39       0.847       0.952: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   178/200    0.891G        1.06        1.39       0.847       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   179/200    0.891G        1.04        1.39       0.845       0.956: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   180/200    0.891G        1.05        1.39       0.849       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   181/200    0.891G        1.05        1.39        0.85       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   182/200    0.891G        1.05        1.39       0.848       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   183/200    0.891G        1.03        1.39       0.847       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   184/200    0.891G        1.04        1.39       0.846       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   185/200    0.891G        1.04        1.39       0.846       0.957: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   186/200    0.891G        1.03        1.39       0.847       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   187/200    0.891G        1.04        1.39       0.848       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   188/200    0.891G        1.04        1.39       0.849       0.958: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   189/200    0.891G        1.04        1.39       0.848       0.959: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   190/200    0.891G        1.05        1.39       0.849       0.959: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   191/200    0.891G        1.04        1.39       0.848       0.959: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   192/200    0.891G        1.04        1.39        0.85       0.959: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   193/200    0.891G        1.02        1.39       0.849       0.959: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   194/200    0.891G        1.03        1.39        0.85        0.96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   195/200    0.891G        1.04        1.39       0.849       0.962: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   196/200    0.891G        1.03        1.39       0.849       0.962: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   197/200    0.891G        1.03        1.39        0.85       0.961: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   198/200    0.891G        1.03        1.39       0.851        0.96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   199/200    0.891G        1.01        1.39        0.85        0.96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n   200/200    0.891G        1.02        1.39       0.851        0.96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n\nTraining complete (0.938 hours)\nResults saved to \u001b[1mruns/train-cls/exp8\u001b[0m\nPredict:         python classify/predict.py --weights runs/train-cls/exp8/weights/best.pt --source im.jpg\nValidate:        python classify/val.py --weights runs/train-cls/exp8/weights/best.pt --data /kaggle/working/datasets/oxford-flowers-102-2\nExport:          python export.py --weights runs/train-cls/exp8/weights/best.pt --include onnx\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp8/weights/best.pt')\nVisualize:       https://netron.app\n\n","output_type":"stream"}]}]}